# -*- coding: utf-8 -*-

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

import os

from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

pd.set_option('display.max_columns', 50)
pd.set_option('display.width', None)
pd.set_option('display.line_width', 200)

dftr = pd.read_csv('datasets/definitive/train/segments_relative_size_10868samples_shorted.csv')
dfts = pd.read_csv('datasets/definitive/test/segments_relative_size_10868samples_shorted.csv')

############ calcul feature size files
fileSizeTrain = pd.DataFrame(columns = ['Id_ext', 'Id', 'size'])
#root = "D:/train"
root = "K:/datamandanga/train"


i = 0
for path, dirs, files in os.walk(root):
    for f in files:
        #print path, f, os.path.getsize( os.path.join( path, f ) )
        fileSizeTrain.loc[i] = [f, f[0:len(f)-4], os.path.getsize( os.path.join( path, f ))]
        #print fileSize.loc[i]
        #print f        
        i = i+1

print fileSizeTrain.head()   
fileSizeTrain.shape
fileSizeTrain.to_csv("datasets/definitive/train/sizeFilesTrain.csv")


fileSizeTest = pd.DataFrame(columns = ['Id_ext', 'Id', 'size'])
root = "K:/datamandanga/test"


i = 0
for path, dirs, files in os.walk(root):
    for f in files:
        #print path, f, os.path.getsize( os.path.join( path, f ) )
        fileSizeTest.loc[i] = [f, f[0:len(f)-4], os.path.getsize( os.path.join( path, f ))]
        #print fileSize.loc[i]
        #print f        
        i = i+1

print fileSizeTest.head()   
fileSizeTest.shape
fileSizeTest.to_csv("datasets/definitive/test/sizeFilesTest.csv")
####################################
##### add feature size file
dfTotalTrain = pd.merge(dftr, fileSizeTrain, how = 'left', left_on = 'sample_id', right_on = 'Id')
dfTotalTrain.head()
dfTotalTrain.drop(['Id', 'Id_ext'], axis = 1, inplace = True)
dfTotalTrain.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis = 1, inplace = True)
dfTotalTrain.head()
dfTotalTrain.shape


colNames = dfTotalTrain.columns.tolist()
print colNames
type(colNames)
print colNames[-3:]

colNames[:-2]
type(colNames[:-2])
colNames[-1]
type(colNames[-1])
colNames[-2]
colNamesSort = colNames[:-2] + [colNames[-1]] + [colNames[-2]]
print colNamesSort
dfTotalTrain = dfTotalTrain[colNamesSort]
dfTotalTrain.head()
####### test
dfTotalTest = pd.merge(dfts, fileSizeTest, how = 'left', left_on = 'sample_id', right_on = 'Id')
dfTotalTest.head()
dfTotalTest.drop(['Id', 'Id_ext'], axis = 1, inplace = True)
dfTotalTest.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis = 1, inplace = True)
dfTotalTest.head()
dfTotalTest.shape


colNames = dfTotalTest.columns.tolist()
print colNames
type(colNames)
print colNames[-3:]

colNames[:-2]
type(colNames[:-2])
colNames[-1]
type(colNames[-1])
colNames[-2]
colNamesSort = colNames[:-2] + [colNames[-1]] + [colNames[-2]]
print colNamesSort

dfTotalTest = dfTotalTest[colNamesSort]
dfTotalTest.shape
dfTotalTest.head()
################ 

dstr = dfTotalTrain.values
dsts = dfTotalTest.values




samplesNames = dftr['sample_id'].values
#print np.shape(samplesNames)

train_X = dstr[:,3:-1]
print train_X
train_y = dstr[:,-1]
print train_y
train_y = train_y[:].astype(int)

test_X = dsts[:,3:-1]
print test_X
test_y = dsts[:,-1]
print test_y
test_y = test_y[:].astype(int)



random_state = 123
n_jobs = -1
verbose = 2
clf = RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=n_jobs, verbose = verbose)

# Start training
#print('training started')
clf.fit(train_X, train_y)
#print('training completed')

#Check on the training set and visualize performance
yhat=clf.predict(train_X)


print "\nTRAINING STATS:"
print "classification accuracy:", metrics.accuracy_score(yhat, train_y)
"""
plt.imshow(metrics.confusion_matrix(train_y, yhat), cmap=plt.cm.binary, interpolation='nearest')
print metrics.confusion_matrix(train_y,yhat)
plt.xlabel('Training confusion matrix')
plt.colorbar()
"""
#Check on the test set and visualize performance
yhat=clf.predict(test_X)
print "TESTING STATS:"
print "classification accuracy:", metrics.accuracy_score(yhat, test_y)

np.set_printoptions(threshold=500)
pp = clf.predict_proba(test_X)
#print str(np.shape(pp))
#print pp[0:20]

#### sum epsilon to probability matrix
pp = pp+0.0003

df1 = pd.DataFrame(data=pp, columns=['Prediction1','Prediction2','Prediction3','Prediction4','Prediction5','Prediction6','Prediction7','Prediction8','Prediction9'])
#print df1



#df = dfts[dfts.]
samplesIdColumnsSerie = dfts.ix[:,2]
finalDf = pd.concat([samplesIdColumnsSerie, df1], axis=1)
finalDf.rename(columns = {'sample_id':'id'}, inplace=True)
finalDf.to_csv('datasets/definitive/trial/segments_size/intersect_size_files_eps_0.0003.csv', index=False)







#print metrics.confusion_matrix(yhat, test_y)
"""
plt.figure()
plt.imshow(metrics.confusion_matrix(test_y, yhat), cmap=plt.cm.binary, interpolation='nearest')
plt.xlabel('Testing confusion matrix')
plt.colorbar()
"""