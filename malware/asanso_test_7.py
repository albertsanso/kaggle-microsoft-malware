# -*- coding: utf-8 -*-

import csv
import random
import os
import os.path
import re

from collections import Counter

import pandas as pd
import numpy as np

def getSamples(seed, bucketSize):    
    samplesStruct = {'my_hash':{}, 'my_list':[]}
    
    #fileFullPath = "resource/trainLabels.csv"
    fileFullPath = "resource/testLabels.csv"
    
    random.seed(seed)
    
    csvfile = open(fileFullPath, 'rb')
    samples = csv.reader(csvfile, delimiter=',')
    
    for row in samples:
        
        sampleName = row[0]
        sampleCategory = row[1]
        samplesStruct['my_hash'][sampleName] = sampleCategory
        samplesStruct['my_list'].append(sampleName)
        
    if (bucketSize > len(samplesStruct['my_list'])):
        bucketSize = len(samplesStruct['my_list'])
    samplesStruct['my_list'] = random.sample(samplesStruct['my_list'], bucketSize)
    
    return samplesStruct
    
def getImportedModules(folder, filename):
    
    filePath = folder + "/" + filename + ".asm"
    
    importModuleFilter = re.compile(
        "^(\.idata):([0-9a-zA-Z]{8,8})"
        "\t+;\sImports from\s(.*)"
    )
    
    importedModulesCounter = Counter()

    if os.path.exists(filePath):
        inputLines = open(filePath).readlines()

        for line in inputLines:
            match1 = importModuleFilter.search(line)
            if (match1):
                module = match1.group(3)
                importedModulesCounter[module] += 1

    return importedModulesCounter

def buildLexicon(corpus):
    lexicon = set()
    for doc in corpus:
        if 'counter' in doc:
            bagOfWords = doc['counter']
            if bagOfWords != None:
                wordsList = list(bagOfWords)
                #print wordsList
                for word in wordsList:
                    lexicon.update([word])
            
    return lexicon 
    
def termFreq(term, doc):
    if (doc != None):
        for myTuple in doc:
            if myTuple[0] == term:
                return myTuple[1]
    return 0
    
def go(theSeed, samplesNumber):
    samples = getSamples(theSeed, samplesNumber)
    samplesDone=0
    
    #folder = "e:/datamandanga/train"
    folder = "e:/datamandanga/test"
    docsList = []
    
    for sampleId in samples['my_list']:
        importedModules = getImportedModules(folder, sampleId)

        struct = {}
        struct['id'] = sampleId
        struct['counter'] = importedModules
        docsList.append(struct)
        
        #print "-----------------------------------------------"
        #print sampleId + " - Category: " + samples['my_hash'][sampleId]
        #print importedModules
        
        pcDone = (float(samplesDone)/samplesNumber)*100
        print "Samples parsed Percent: "+str(pcDone)
        samplesDone += 1
        
    matrix = []
    vocabulary = buildLexicon(docsList)
    for docStruct in docsList:
        doc = docStruct['counter'].most_common()
        sampleId = docStruct['id']
        
        tf_vector = [termFreq(word, doc) for word in vocabulary]
        tf_vector.insert(0, sampleId)
        tf_vector.append(samples['my_hash'][sampleId])    
        matrix.append(tf_vector)
        
    vocabulary = list(vocabulary)
    vocabulary.insert(0, 'id')
    vocabulary.append('category')
    
    docTermMatrix = (vocabulary, matrix)
    
    
    vocabulary, nDocTermMatrix = np.asarray(docTermMatrix)
    df = pd.DataFrame(data=nDocTermMatrix, columns=vocabulary)
    

    #df.to_csv("datasets/definitive/train/win32_imports_count_samples_"+str(samplesNumber)+".csv", index=False)
    df.to_csv("datasets/definitive/test/win32_imports_count_samples_"+str(samplesNumber)+".csv", index=False)

# -----------------------------------------------------------------------------


go("my_seed", 11000)








