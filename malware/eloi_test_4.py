# -*- coding: utf-8 -*-

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import sklearn.ensemble as sken
import sklearn.naive_bayes as sknb
import sklearn.neighbors as skne
import DecisionTreeClassifier from sklearn.tree 


#from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics


random_state = 123
n_jobs = -1
verbose = 2


#def_classifier={ "classifier": "RandomForestClassifier", 
#              "params": {"n_estimators":10000,"random_state":random_state, "n_jobs": n_jobs, "verbose" : verbose}
#}

# Uncomment For ExtrTreeClassifier
#def_classifier={ "classifier": "ExtraTreesClassifier", 
#              "params": {"n_estimators":10000,"random_state":random_state, "n_jobs": n_jobs, "verbose" : verbose}
#            }


#### Uncomment For AdaboostClassifier
def_classifier={ "classifier": "AdaBoostClassifier", 
             "params": {"base_estimator":DecisionTreeClassifier(max_depth=2),"learning_rate":1.5,"n_estimators":10000,"random_state":random_state }
}


#def_classifier={ "classifier": "MultinomialNB", 
#             "params": {}
#}


#def_classifier={ "classifier": "KNeighborsClassifier", 
#            "params": {}
#}



#### Uncomment For AdaboostClassifier with RF base_estimator
#randomforest = sken.RandomForestClassifier(n_estimators=1000, random_state=random_state, n_jobs=n_jobs, verbose = verbose)
#def_classifier={ "classifier": "AdaBoostClassifier", 
#              "params": {"base_estimator"=randomforest,"n_estimators":1000,"random_state":random_state, "n_jobs": n_jobs, "verbose" : verbose}}

args = def_classifier["params"]
#classifier = getattr(sken, def_classifier["classifier"])
classifier = getattr(sken, def_classifier["classifier"])
clf  = classifier(**args)


pd.set_option('display.max_columns', 50)
pd.set_option('display.width', None)
pd.set_option('display.line_width', 200)

dftr = pd.read_csv('datasets/definitive/train/concat_features_sections_fileSize_train.csv')
dfts = pd.read_csv('datasets/definitive/test/concat_features_sections_fileSize_test.csv')

dstr = dftr.values
dsts = dfts.values

samplesNames = dftr['sample_id'].values
#print np.shape(samplesNames)

train_X = dstr[:,3:-1]
print train_X
train_y = dstr[:,-1]
print train_y
train_y = train_y[:].astype(int)

test_X = dsts[:,3:-1]
print test_X
test_y = dsts[:,-1]
print test_y
test_y = test_y[:].astype(int)






#RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=n_jobs, verbose = verbose)

# Start training
#print('training started')
clf.fit(train_X, train_y)
#print('training completed')

#Check on the training set and visualize performance
yhat=clf.predict(train_X)


print "\nTRAINING STATS:"
print "classification accuracy:", metrics.accuracy_score(yhat, train_y)
"""
plt.imshow(metrics.confusion_matrix(train_y, yhat), cmap=plt.cm.binary, interpolation='nearest')
print metrics.confusion_matrix(train_y,yhat)
plt.xlabel('Training confusion matrix')
plt.colorbar()
"""
#Check on the test set and visualize performance
yhat=clf.predict(test_X)
print "TESTING STATS:"
print "classification accuracy:", metrics.accuracy_score(yhat, test_y)

np.set_printoptions(threshold=500)
pp = clf.predict_proba(test_X)
#print str(np.shape(pp))
#print pp[0:20]

#### sum epsilon to probability matrix
pp = pp+0.0001
print pp

df1 = pd.DataFrame(data=pp, columns=['Prediction1','Prediction2','Prediction3','Prediction4','Prediction5','Prediction6','Prediction7','Prediction8','Prediction9'])
print df1



#df = dfts[dfts.]
samplesIdColumnsSerie = dfts.ix[:,1]
finalDf = pd.concat([samplesIdColumnsSerie, df1], axis=1)
finalDf.rename(columns = {'sample_id':'id'}, inplace=True)
finalDf.to_csv('datasets/definitive/trial/segments_size/concat_segsize_totsize_1000estimators_0001epsilon_Adaboost1000.csv', index=False)






#print metrics.confusion_matrix(yhat, test_y)
"""
plt.figure()
plt.imshow(metrics.confusion_matrix(test_y, yhat), cmap=plt.cm.binary, interpolation='nearest')
plt.xlabel('Testing confusion matrix')
plt.colorbar()
"""