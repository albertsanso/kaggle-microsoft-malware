# -*- coding: utf-8 -*-
"""
Created on Thu Apr 09 08:16:22 2015

@author: Jordi
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.cross_validation import train_test_split

def combineFilesById(file_left, file_right, rightColumn, leftColumn):
    dfLeft = pd.read_csv(file_left)
    dfRigt = pd.read_csv(file_right)
    dfRigt.head()
    dfRight = pd.read_csv(file_right)
    mergedDf = pd.merge(dfLeft, dfRigt, how='left', left_on=leftColumn, right_on=rightColumn)
    return combineDataFramesById(dfLeft, dfRight, rightColumn, leftColumn)
    
def combineDataFramesById(dfLeft, dfRight, rightColumn, leftColumn):
    mergedDf = pd.merge(dfLeft, dfRight, how='left', left_on=leftColumn, right_on=rightColumn)
    #mergedDf.drop('category_x', axis=1, inplace=True)
    mergedDf.drop(rightColumn, axis=1, inplace=True)
    mergedDf.rename(columns = {'category_y':'category'}, inplace=True)
    return mergedDf

def matrixWithoutDiag(A):
    diagA = np.diagonal(A)
    B = A-np.diag(diagA)
    return B 

def minLogloss(ytest, yhat_p, pas):    
    n = int(0.002/pas)
    limplot = int(n/10)
    logloss = np.zeros((n,2))    
    for j in xrange(n):
        logloss[j,0] = j*pas        
        logloss[j,1] = metrics.log_loss(y_test, yhat_p+(j*pas),eps=1e-15)
    plt.title = 'title'
    plt.plot(logloss[:limplot,0],logloss[:limplot,1])
    #title = 'Evol logloss al sumar una constant a la matriu de prob'    
    plt.ylabel('logloss')
    plt.xlabel('eps sumat')
    minimLogloss = np.argmin(logloss[:, 1])
    return logloss[minimLogloss]

dftr = pd.read_csv("C:/Users/Jordi/Documents/kaggle_malware/work/malware/2nd_iteration/datasets/hybrid/train/SSabs+FSasm+ADN1abs+ADN2abs_Train.csv")#, index = False)
dfts = pd.read_csv("C:/Users/Jordi/Documents/kaggle_malware/work/malware/2nd_iteration/datasets/hybrid/test/SSabs+FSasm+ADN1abs+ADN2abs_Test.csv")#, index = False)

dstr = dftr.values
dsts = dfts.values
dstr

X_train = dstr[:,2:-1]
X = dstr[:,2:-1]
y = dstr[:,-1]
y = y[:].astype(int)

PRC = 0.1
N = int(1.0/PRC)
acc=np.zeros((N,))
matLogloss = np.zeros((N,2))
totalErrors = np.zeros((9,9))




for i in xrange(N):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=PRC)
    #clf = RandomForestClassifier(n_estimators = 250, n_jobs = 4, random_state=123, verbose = 2)
    clf = RandomForestClassifier(n_estimators = 50, random_state=123, verbose = 1, oob_score = True)    
    #clf = RandomForestClassifier(n_estimators = 250, random_state=123, verbose = 1)    
    clf.fit(X_train,y_train)
    yhat=clf.predict(X_test)
    yhat_p = clf.predict_proba(X_test)
    print "results of iteration", i    
    print yhat_p
    #print metrics.log_loss(y_test, yhat_p,eps=1e-15)
    #print metrics.log_loss(y_test, yhat_p+0.0001,eps=1e-15)    
    matLogloss[i] = minLogloss(y_test, yhat_p, 0.00001)        
    acc[i] = metrics.accuracy_score(yhat, y_test)
    print metrics.classification_report(y_test, yhat)
    #A = metrics.confusion_matrix(y_test, yhat)
    B= matrixWithoutDiag(metrics.confusion_matrix(y_test, yhat))
    #plt.imshow(B, cmap=plt.cm.binary, interpolation='nearest')
    #plt.xlabel('Training confusion matrix')
    #plt.colorbar()
    print metrics.confusion_matrix(y_test,yhat)
    print "end of iteration",i
    totalErrors += B

plt.imshow(totalErrors, cmap=plt.cm.binary, interpolation='nearest')
print totalErrors
plt.xlabel('Training confusion matrix')
plt.colorbar()
print "mitjana del logloss", matLogloss[:,1].mean(), 'diferència màxima', matLogloss[:,1].max() - matLogloss[:,1].min()
print "mitjana del eps sumat", matLogloss[:,0].mean(), 'diferència màxima', matLogloss[:,0].max() - matLogloss[:,0].min()
 
print matLogloss
print acc
print "Mean expected error: "+str(np.mean(acc[0]))


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=PRC)
efficienceClassifier = pd.DataFrame(columns = ['n_estimators','train_accuracy','test_accuracy'])
for i in range(1,500):
    clf = RandomForestClassifier(n_estimators = i,warm_start = True, random_state=123, verbose = 0)
    clf.fit(X_train, y_train)
    yhat_train = clf.predict(X_train)
    yhat_test = clf.predict(X_test)    
    efficienceClassifier.loc[i-1] = [i,metrics.accuracy_score(yhat_train, y_train),metrics.accuracy_score(yhat_test, y_test)]    
    print "Número d'estimadors:", i        
    print "classification accuracy:", metrics.accuracy_score(yhat_train, y_train)
    print "classification accuracy:", metrics.accuracy_score(yhat_test, y_test)    
   
efficienceClassifier.plot(x = 'n_estimators', y = ['train_accuracy', 'test_accuracy']) 
efficienceClassifier.to_csv('D:/RF_1_to_100_sections_size.csv')
efficienceClassifier.plot(x = 'n_estimators', y = ['train_accuracy', 'test_accuracy'], ylim = [0.985,0.995] )
efficienceClassifier.plot? 
"""
buscar fitxers erronis
"""
X = dstr[:,1:-1]
y = dstr[:,-1]
perm = np.random.permutation(y.size)
PRC = 0.7
split_point = int(np.ceil(y.shape[0]*PRC))

X_train_all = X[perm[:split_point].ravel(),:]
y_train = y[perm[:split_point].ravel()]
y_train = y_train[:].astype(int)
X_train = X_train_all[:,1:]

X_test_all = X[perm[split_point:].ravel(),:]
y_test = y[perm[split_point:].ravel()]
y_test = y_test[:].astype(int)
X_test = X_test_all[:,1:]


clf = RandomForestClassifier(n_estimators = 250, random_state=123, verbose = 1, oob_score = True)    

clf.fit(X_train,y_train)
yhat=clf.predict(X_test)
yhat_p = clf.predict_proba(X_test)

print yhat_p
#print metrics.log_loss(y_test, yhat_p,eps=1e-15)
#print metrics.log_loss(y_test, yhat_p+0.0001,eps=1e-15)    
matLogloss = minLogloss(y_test, yhat_p, 0.00001)        
metrics.accuracy_score(yhat, y_test)
print metrics.classification_report(y_test, yhat)
#A = metrics.confusion_matrix(y_test, yhat)
B= matrixWithoutDiag(metrics.confusion_matrix(y_test, yhat))
#plt.imshow(B, cmap=plt.cm.binary, interpolation='nearest')
#plt.xlabel('Training confusion matrix')
#plt.colorbar()
print metrics.confusion_matrix(y_test,yhat)
print "end of iteration",i


def get_files_predictions(dataset_test, ytest, yhat):
    file_predictions = pd.DataFrame(columns = ['Id','true_category','predict_category'])    
    for i in xrange(len(y_test)):
        file_predictions.loc[i-1] = [X_test_all[i,0],y_test[i],yhat[i]]
    file_predictions.to_csv("C:/Users/Jordi/Documents/kaggle_malware_backup/CSV/file_predictions.csv",index = False)    
    return file_predictions

def get_files_predictions_errors(dataset_test, ytest, yhat):
    file_predictions_errors = pd.DataFrame(columns = ['Id','true_category','predict_category'])    
    for i in xrange(len(y_test)):
        if y_test[i] != yhat[i]:
            file_predictions_errors.loc[i-1] = [X_test_all[i,0],y_test[i],yhat[i]]
    file_predictions_errors.to_csv("C:/Users/Jordi/Documents/kaggle_malware_backup/CSV/file_predictions_errors.csv",index = False)    
    return file_predictions_errors
    
get_files_predictions(X_test_all, y_test, yhat)
files_predictions =get_files_predictions(X_test_all, y_test, yhat)
files_predictions_errors =get_files_predictions_errors(X_test_all, y_test, yhat)
files_predictions['Id']
files_predictions_errors
prova = combineDataFramesById(dftr,files_predictions, 'Id','Id' )
a = prova[prova['true_category'] != prova['predict_category']]


col = files_predictions_errors.columns
a = pd.DataFrame(columns = col)
j = 0
for i in files_predictions_errors['Id']:
    temp = dftr[dftr['Id'] == i]  
    print temp

prova.to_csv("C:/Users/Jordi/Documents/kaggle_malware_backup/CSV/dataset+file_predictions_categorys.csv",index = False)    

for i in range(files_predictions_errors.shape[0]):
        
    temp = dftr[dftr['Id'] == i['Id']]  
    print temp    
    

totalErrors += B

plt.imshow(totalErrors, cmap=plt.cm.binary, interpolation='nearest')
print totalErrors
plt.xlabel('Training confusion matrix')
plt.colorbar()













dstr
dsts

X_test = X[perm[split_point:].ravel(),:]
y_test = y[perm[split_point:].ravel()]
y_test = y_test[:].astype(int)
print y_test.shape
"""
random_state = 123
n_jobs = -1
verbose = 2
N = 100
#clf = RandomForestClassifier(n_estimators=10, random_state=random_state, n_jobs=n_jobs, verbose = verbose)
clf = RandomForestClassifier(n_estimators = 250, random_state=random_state, verbose = 2)
#### 250 estimators va de conya
# Start training
print('training started')
clf.fit(X_train, y_train)
print('training completed')

#Check on the training set and visualize performance
yhat=clf.predict(X_train)


print "\nTRAINING STATS:"
print "classification accuracy:", metrics.accuracy_score(yhat, y_train)
#Check on the test set and visualize performance
yhat=clf.predict(X_test)
print "TESTING STATS:"
print "classification accuracy:", metrics.accuracy_score(yhat, y_test)



efficienceClassifier = pd.DataFrame(columns = ['n_estimators','train_accuracy','test_accuracy'])
for i in range(1,250):
    clf = RandomForestClassifier(n_estimators = i, random_state=random_state, verbose = 0)
    clf.fit(X_train, y_train)
    yhat_train = clf.predict(X_train)
    yhat_test = clf.predict(X_test)    
    efficienceClassifier.loc[i-1] = [i,metrics.accuracy_score(yhat_train, y_train),metrics.accuracy_score(yhat_test, y_test)]    
    print "Número d'estimadors:", i        
    print "classification accuracy:", metrics.accuracy_score(yhat_train, y_train)
    print "classification accuracy:", metrics.accuracy_score(yhat_test, y_test)    
    
efficienceClassifier.plot(x = 'n_estimators', y = ['train_accuracy', 'test_accuracy']) 


efficienceClassifier = pd.DataFrame(columns = ['n_estimators','train_accuracy','test_accuracy'])
for i in range(1,1000):
    clf = RandomForestClassifier(n_estimators = i, random_state=random_state, verbose = 0)
    clf.fit(X_train, y_train)
    yhat_train = clf.predict(X_train)
    yhat_test = clf.predict(X_test)    
    efficienceClassifier.loc[i-1] = [i,metrics.accuracy_score(yhat_train, y_train),metrics.accuracy_score(yhat_test, y_test)]    
    print "Número d'estimadors:", i        
    print "classification accuracy:", metrics.accuracy_score(yhat_train, y_train)
    print "classification accuracy:", metrics.accuracy_score(yhat_test, y_test)    
    
efficienceClassifier.plot(x = 'n_estimators', y = ['train_accuracy', 'test_accuracy']) 
efficienceClassifier.plot? 
"""  