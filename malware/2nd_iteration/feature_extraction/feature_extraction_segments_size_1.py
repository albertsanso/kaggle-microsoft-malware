# -*- coding: utf-8 -*-

import csv
import random
import os
import os.path
import re

from collections import Counter

import pandas as pd
import numpy as np

SEGMENT_NAMES_LIST = set(["text", "bss", "rdata", "data", "rsrc", "edata", "idata", "pdata", "debug"])

def getSamples(seed, bucketSize, descriptorPath):    
    samplesStruct = {'my_hash':{}, 'my_list':[]}
    
    random.seed(seed)
    
    csvfile = open(descriptorPath, 'rb')
    samples = csv.reader(csvfile, delimiter=',')
    
    for row in samples:
        
        sampleName = row[0]
        sampleCategory = row[1]
        if sampleCategory == 'Class':
            continue
            
        samplesStruct['my_hash'][sampleName] = sampleCategory
        samplesStruct['my_list'].append(sampleName)
        
    if (bucketSize > len(samplesStruct['my_list'])):
        bucketSize = len(samplesStruct['my_list'])
        
    samplesStruct['my_list'] = random.sample(samplesStruct['my_list'], bucketSize)
    
    return samplesStruct

def simpleNormalizeBag(bag):
    normalizedBag = []
    
    if len(bag) > 0:
        labels, values = zip(*bag)
        totalSum = float(sum(values))
        
        for tpl in bag:
            occurrences = tpl[1]
            normalizedOccurrences = occurrences/totalSum
            normalizedTuple = (tpl[0], normalizedOccurrences)
            normalizedBag.append(normalizedTuple)
        
    return normalizedBag
    
def getSectionStats_FromFile(folder, filename):
    
    filepath = folder + "/" + filename + ".asm"
    
    if os.path.exists(filepath):
        with open(filepath) as infile:
            inputLines = infile.readlines()
            #print inputLines
            return getSectionsStats_FromArray(inputLines, filename)
    else:
        return None

def mapSectionAlias(name):
    aliases = {}
    aliases['CODE'] = 'text'
    
    mapped = aliases.get(name, name)
    return mapped
    
def getSectionsStats_FromArray(lines, filename):
    
    bag = Counter()
    
    expr1 = re.compile(
        "^\.{0,1}(\w+):(.*)"
    )
    
    for line in lines:
        match1 = expr1.search(line)
        if match1:
            section = match1.group(1)
            if section != 'HEADER':
                #print section
                section = mapSectionAlias(section)
                
                if (section in SEGMENT_NAMES_LIST):
                    bag[section] += 1
                else:
                    bag["OTHER"] += 1
    
    sectionsDistribution = simpleNormalizeBag(bag.items())
    assemblyStruct = {}
    assemblyStruct['counter'] = sectionsDistribution
    assemblyStruct['id'] = filename
    
    return assemblyStruct

def buildLexicon(corpus):
    lexicon = set()
    for doc in corpus:
        if 'counter' in doc:
            bagOfWords = doc['counter']
            if bagOfWords != None:
                wordsList = list(bagOfWords)
                #print wordsList
                for word in wordsList:
                    lexicon.update([word[0]])
            
    return lexicon 

def termFreq(term, doc):
    if (doc != None):
        for myTuple in doc:
            if myTuple[0] == term:
                return myTuple[1]
    return 0
    
def launch(theSeed, samplesNumber, descriptorPath, folder, outputName):
    samples = getSamples(theSeed, samplesNumber, descriptorPath)
    samplesDone = 0
    
    statsList = []
    for sampleId in samples['my_list']:
        stat = getSectionStats_FromFile(folder, sampleId)
        statsList.append(stat)
    
        pcDone = (float(samplesDone)/samplesNumber)*100
        print "Samples parsed Percent: "+str(pcDone)
        samplesDone += 1

    matrix = []
    vocabulary = buildLexicon(statsList)
    
    for docStruct in statsList:
        #doc = docStruct['counter'].most_common()
        doc = docStruct['counter']
        #print docStruct
        sampleId = docStruct['id']
        
        tf_vector = [termFreq(word, doc) for word in vocabulary]
        tf_vector.insert(0, sampleId)
        tf_vector.append(samples['my_hash'][sampleId])    
        matrix.append(tf_vector)
        
    vocabulary = list(vocabulary)
    vocabulary.insert(0, 'id')
    vocabulary.append('category')
    
    docTermMatrix = (vocabulary, matrix)
    
    vocabulary, nDocTermMatrix = np.asarray(docTermMatrix)
    df = pd.DataFrame(data=nDocTermMatrix, columns=vocabulary)
    
    df.to_csv(outputName, index=False)
    
# ------------------------------------------------------------------------

def extractTrainFeatures():
    seed = "my_seed"    
    folder = "D:/datamandanga/train"
    samplesNumber = 11000
    descriptor = "../../resource/trainLabels.csv"
    outputName = "../datasets/single/train/train_sections_size.csv"
    
    launch(seed, samplesNumber, descriptor, folder,outputName)
    
def extractTestFeatures():
    seed = "my_seed"    
    folder = "D:/datamandanga/test"
    samplesNumber = 11000
    descriptor = "../../resource/testLabels.csv"
    outputName = "../datasets/single/test/test_sections_size.csv"
    
    launch(seed, samplesNumber, descriptor, folder,outputName)
    
# ------------------------------------------------------------------------
    
extractTrainFeatures()